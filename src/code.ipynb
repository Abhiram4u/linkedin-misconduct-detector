{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f93f3b-eb7f-45e9-ad18-906a6b0c16e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text files merged into C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\merged_file\\merged_chat_data.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_text_from_subfolders(main_folder, output_file=r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\merged_file\\merged_chat_data.txt\"):\n",
    "   \n",
    "    ## Extracted text from each text file inside  subfolders within the main folder.\n",
    "    \n",
    "    # main_folder-The directory containing multiple numbered subfolders.\n",
    "    # output_file-The filename to store merged text data.\n",
    "    \n",
    "    all_texts = []\n",
    "\n",
    "    # List all subfolders\n",
    "    for subfolder in sorted(os.listdir(main_folder)):  \n",
    "        subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "        \n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Get the first text file inside the subfolder\n",
    "            text_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "            for text_file in text_files:\n",
    "                file_path = os.path.join(subfolder_path, text_file)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        all_texts.append(file.read())  # Store text content\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    # Save merged text into a single file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "        merged_file.write(\"\\n\".join(all_texts))\n",
    "    \n",
    "    print(f\"All text files merged into {output_file}\")\n",
    "\n",
    "main_folder = r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\GMT20240125-121850_RecordingnewChat\"\n",
    "extract_text_from_subfolders(main_folder)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645a4fe-13a2-49e7-a11d-8f292b55566f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78919fbe-42ec-4250-8441-e03585e447a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25826428-a862-4eaa-bceb-e2250860e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 6404 valid LinkedIn profiles.\n",
      "Extracted 128 valid unique LinkedIn profiles.\n",
      "Extracted 187 invalid LinkedIn profiles.\n",
      "Identified 12 instances of misconduct Zoom links.\n",
      "Identified 1 instances of unique misconduct Zoom links.\n",
      "extracted 6591 overall linkedin profiles (including duplicates)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_info(chat_file):\n",
    "   \n",
    "    \n",
    "    ##chat_file- Path to the merged chat data file.\n",
    "   \n",
    "    \n",
    "    # Define regex patterns\n",
    "    \n",
    "    linkedin_pattern = r\"(https?://)?(www.)?(linkedin.com/in/[a-zA-Z0-9-]+)\"\n",
    "    all_url_pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:linkedin\\.com\\/in\\/[a-zA-Z0-9-_]+|[\\w.-]+\\.[a-zA-Z]{2,}[\\w/?&=-]*)\"\n",
    "    zoom_pattern = r\"(https?:\\/\\/)?(www\\.)?zoom\\.us\\/j\\/[0-9]+\"\n",
    "\n",
    "    # Data structures to store URLs\n",
    "    \n",
    "    valid_linkedin_profiles = []\n",
    "    valid_unique_linkedin_profiles = set()\n",
    "    invalid_linkedin_profiles = []\n",
    "    misconduct_cases = []\n",
    "    unique_misconduct_cases = set()\n",
    "\n",
    "\n",
    "    # Read chat data file\n",
    "    with open(chat_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Extract all URLs (valid and invalid)\n",
    "            all_urls = re.findall(all_url_pattern, line)\n",
    "            for url in all_urls:\n",
    "                full_url = ''.join(url)  # Join components of URL, if any\n",
    "\n",
    "                # Classify LinkedIn URLs as valid or invalid\n",
    "                if re.match(linkedin_pattern, full_url):\n",
    "                    valid_linkedin_profiles.append(full_url) \n",
    "                    valid_unique_linkedin_profiles.add(full_url) # Valid LinkedIn URL\n",
    "                else:\n",
    "                    if \"linkedin.com\" in full_url and not re.match(linkedin_pattern, full_url):\n",
    "                        invalid_linkedin_profiles.append(full_url)\n",
    "                if \"zoom.us\" in full_url and \"zoom\" in full_url:\n",
    "                    misconduct_cases.append(full_url)\n",
    "                    unique_misconduct_cases.add(full_url)\n",
    "                    \n",
    "\n",
    "             \n",
    "                \n",
    "                \n",
    "\n",
    "            # Identify misconduct (Zoom links)\n",
    "            ##if re.search(zoom_pattern, line):\n",
    "                ##misconduct_cases.append(line.strip())\n",
    "\n",
    "    # Define output file paths\n",
    "    valid_unique_linkedin_file = r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\Extracted_results\\valid_unique_linkedin_profiles.txt\"\n",
    "    valid_linkedin_file = r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\Extracted_results\\valid_linkedin_profiles.txt\"\n",
    "    invalid_linkedin_file = r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\Extracted_results\\invalid_linkedin_profiles.txt\"\n",
    "    misconduct_file = r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\Extracted_results\\misconduct_zoom_links.txt\"\n",
    "    unique_misconduct_file = r\"C:\\Users\\abhir\\OneDrive\\Desktop\\interships tasks\\TASK 2\\Extracted_results\\unique_misconduct_zoom_links.txt\"\n",
    "\n",
    "    \n",
    "    # Save valid LinkedIn profiles\n",
    "    with open(valid_linkedin_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(valid_linkedin_profiles))\n",
    "        \n",
    "    with open(valid_unique_linkedin_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(valid_unique_linkedin_profiles))\n",
    "\n",
    "    # Save invalid LinkedIn profiles\n",
    "    with open(invalid_linkedin_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(invalid_linkedin_profiles))\n",
    "\n",
    "    # Save misconduct cases (Zoom links)\n",
    "    with open(misconduct_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(misconduct_cases))\n",
    "\n",
    "     # Save unique misconduct cases (Zoom links)\n",
    "    with open(unique_misconduct_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(unique_misconduct_cases))\n",
    "\n",
    "    # Output results\n",
    "    \n",
    "    print(f\"Extracted {len(valid_linkedin_profiles)} valid LinkedIn profiles.\")\n",
    "    print(f\"Extracted {len(valid_unique_linkedin_profiles)} valid unique LinkedIn profiles.\")\n",
    "    print(f\"Extracted {len(invalid_linkedin_profiles)} invalid LinkedIn profiles.\")\n",
    "    print(f\"Identified {len(misconduct_cases)} instances of misconduct Zoom links.\")\n",
    "    print(f\"Identified {len(unique_misconduct_cases)} instances of unique misconduct Zoom links.\")\n",
    "\n",
    "   # total linkedin profile (including valid,invalid,duplicates)\n",
    "    print(f\"extracted {len(valid_linkedin_profiles)+len(invalid_linkedin_profiles)} overall linkedin profiles (including duplicates)\")\n",
    "\n",
    "extract_info(\"C:/Users/abhir/OneDrive/Desktop/interships tasks/TASK 2/merged_file/merged_chat_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1cec12-9bc0-4304-a209-551ef0b8773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c570ce-0573-44f4-bc19-b436812331d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4b35e-b8ce-452a-98f5-c2c84ade5c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c949dee2-7f16-4083-b6ca-b0d592941905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ediii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47036f0b-cf99-4c1c-94d6-1ed7b70d7517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b57d7-2cfc-4f39-8035-3aecb8184fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
